{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_MetricLearning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRro4AC77SNb"
      },
      "source": [
        "Install pytorch-metric-lightning and faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMde13VLDjiH"
      },
      "source": [
        "!pip install pytorch-metric-learning\n",
        "!pip install faiss-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7POoBkgD7a3Q"
      },
      "source": [
        "Imports...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8qsBm5qbRuX"
      },
      "source": [
        "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
        "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
        "from torchvision import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.datasets import CIFAR10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjUL5wKI7r-V"
      },
      "source": [
        "Create our transform.\n",
        "\n",
        "Specify our batch size, data loaders, and other miscellanea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUDZ5qkuSRBh",
        "collapsed": true
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "dataset1 = CIFAR10('./', train=True, download=True, transform=transform)\n",
        "dataset2 = CIFAR10('./', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size)\n",
        "\n",
        "accuracy_calculator = AccuracyCalculator(include = (\"precision_at_1\",), k = 1)\n",
        "def get_all_embeddings(dataset, model):\n",
        "    tester = testers.BaseTester()\n",
        "    return tester.get_all_embeddings(dataset, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJv3YTUn7_FA"
      },
      "source": [
        "<h2>Your homework assignment</h2>\n",
        "\n",
        "Review the code for different losses provided by pytorch-metric-learning: https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/src/pytorch_metric_learning/losses (click through a few of them to get a hold of how the code looks, the types of parameters you can change, what the different losses are, etc.)\n",
        "\n",
        "Select three different losses (not including TripletMarginLoss) to train with. Explore the different parameters for those loss functions (e.g., train with different choices).\n",
        "\n",
        "For each of the different loss functions you are considering, copy the cell below (there should be *four* different cells when you're done -- the one below which provides a \"baseline\" and then your 3 experiments).\n",
        "\n",
        "\n",
        "\n",
        "**Changing parameters:**\n",
        "\n",
        "Parameters that you can change include the embedding size (output_sz), learning rate, margin, the \"type_of_triplets\" being considered by the miner (you may also explore other miners, although that is not required -- if you do use a different miner, please discuss this in your writeup).\n",
        "\n",
        "Some caveats for changing parameters:\n",
        "\n",
        "You may change the embedding size from 64, but you should use the *same* embedding size for your three experiments\n",
        "\n",
        "Please *do not* change the number of epochs. I recognize that only 2 epochs may prevent your results from converging, but it will give an apples to apples comparison and keep things from taking too long to train.\n",
        "\n",
        "\n",
        "**Deliverable:**\n",
        "\n",
        "Provide a write up (~1-2 pages, whatever medium you want) which reports on the achieved accuracy of the different approaches relative to the baseline TripletMarginLoss provided. What parameters did you change and what values did you try? Which achieved the best results? Which of the losses achieved the best accuracy? Where do you think that improvement in accuracy came from?\n",
        "\n",
        "Upload that write up and your code.\n",
        "\n",
        "75 points will be assigned based on your write up. 25 points for functional code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI0ymq2-Si8L"
      },
      "source": [
        "# you can change these\n",
        "output_sz = 64 # make sure this is the same across all experiments (you can experiment with different values)\n",
        "learning_rate = 0.01\n",
        "margin = 0.2\n",
        "\n",
        "# don't change this\n",
        "num_epochs = 2\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(512, output_sz) # this \"chops off\" the pre-trained fc layer, and replaces it with a new random one that's the desired size\n",
        "\n",
        "model = model.to('cuda') # put this on the gpu\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "distance = distances.CosineSimilarity()\n",
        "\n",
        "# here are the miner and loss function (the thing you'll be changing to different losses)\n",
        "miner = miners.TripletMarginMiner(distance = distance, type_of_triplets = \"all\")\n",
        "loss_func = losses.TripletMarginLoss(margin = margin, distance = distance)\n",
        "\n",
        "# train\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(data)\n",
        "        indices_tuple = miner(embeddings, labels)\n",
        "        loss = loss_func(embeddings, labels, indices_tuple)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(\"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(epoch, batch_idx, loss, miner.num_triplets))\n",
        "\n",
        "    train_embeddings, train_labels = get_all_embeddings(dataset1, model)\n",
        "    test_embeddings, test_labels = get_all_embeddings(dataset2, model)\n",
        "    print(\"Computing accuracy\")\n",
        "    accuracies = accuracy_calculator.get_accuracy(test_embeddings, \n",
        "                                                train_embeddings,\n",
        "                                                test_labels,\n",
        "                                                train_labels,\n",
        "                                                False)\n",
        "    print(\"Epoch: {} | Test set accuracy (Precision@1) = {}\".format(epoch, accuracies[\"precision_at_1\"]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}